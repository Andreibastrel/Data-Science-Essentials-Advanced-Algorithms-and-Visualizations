{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Machine Learning Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non linear algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_svmlight_file\n",
    "X_train, y_train = load_svmlight_file('ijcnn1.bz2')\n",
    "first_rows = 2500\n",
    "X_train, y_train = X_train[:first_rows,:], y_train[:first_rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anc\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC with rbf kernel -> cross validation accuracy: mean = 0.910 std = 0.001\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "hypothesis = SVC(kernel='rbf', random_state=101)\n",
    "scores = cross_val_score(hypothesis, X_train, y_train, cv=5, scoring='accuracy')\n",
    "print (\"SVC with rbf kernel -> cross validation accuracy: mean = %0.3f std = %0.3f\" %\n",
    "       (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "covertype_dataset = pickle.load(open( \"covertype_dataset.pickle\", \"rb\" ))\n",
    "covertype_X = covertype_dataset.data[:25000,:]\n",
    "covertype_y = covertype_dataset.target[:25000] -1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original dataset: (581012, 54)\n",
      "sub-sample: (25000, 54)\n",
      "target freq: [('Spruce/Fir', 9107), ('Lodgepole Pine', 12122), ('Ponderosa Pine', 1583), ('Cottonwood/Willow', 120), ('Aspen', 412), ('Douglas-fir', 779), ('Krummholz', 877)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "covertypes = ['Spruce/Fir', 'Lodgepole Pine', 'Ponderosa Pine', \n",
    "        'Cottonwood/Willow', 'Aspen', 'Douglas-fir', 'Krummholz']\n",
    "print ('original dataset:', covertype_dataset.data.shape)\n",
    "print ('sub-sample:', covertype_X.shape)\n",
    "print('target freq:', list(zip(covertypes,np.bincount(covertype_y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC -> cross validation accuracy: mean = 0.646 std = 0.018\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import cross_val_score, StratifiedKFold\n",
    "from sklearn.svm import LinearSVC\n",
    "hypothesis = LinearSVC(dual=False, class_weight='balanced')\n",
    "cv_strata = StratifiedKFold(covertype_y, n_folds=3, shuffle=True, random_state=101)\n",
    "scores = cross_val_score(hypothesis, covertype_X, covertype_y, cv=cv_strata, scoring='accuracy')\n",
    "print (\"LinearSVC -> cross validation accuracy: mean = %0.3f std = %0.3f\" %\n",
    "       (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "X_train, y_train = pickle.load(open( \"cadata.pickle\", \"rb\" ))\n",
    "from sklearn.preprocessing import scale\n",
    "first_rows = 2000\n",
    "X_train = scale(X_train[:first_rows,:].toarray())\n",
    "y_train = y_train[:first_rows]/10**4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVR -> cross validation accuracy: mean = -4.618 std = 0.347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anc\\lib\\site-packages\\sklearn\\metrics\\scorer.py:100: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "E:\\Anc\\lib\\site-packages\\sklearn\\metrics\\scorer.py:100: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "E:\\Anc\\lib\\site-packages\\sklearn\\metrics\\scorer.py:100: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.svm import SVR\n",
    "hypothesis = SVR()\n",
    "scores = cross_val_score(hypothesis, X_train, y_train, cv=3,scoring='mean_absolute_error')\n",
    "print (\"SVR -> cross validation accuracy: mean = %0.3f std = %0.3f\" %\n",
    "       (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anc\\lib\\site-packages\\sklearn\\grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters {'gamma': 0.1, 'C': 100}\n",
      "Cross validation accuracy: mean = 0.998\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "X_train, y_train = load_svmlight_file('ijcnn1.bz2')\n",
    "first_rows = 2500\n",
    "X_train, y_train = X_train[:first_rows,:], y_train[:first_rows]\n",
    "hypothesis = SVC(kernel='rbf', random_state=101)\n",
    "search_dict = {'C': [0.01, 0.1, 1, 10, 100], \n",
    "               'gamma': [0.1, 0.01, 0.001, 0.0001]}\n",
    "search_func = RandomizedSearchCV(estimator=hypothesis, \n",
    "              param_distributions=search_dict, n_iter=10, scoring='accuracy',\n",
    "              n_jobs=-1, iid=True, refit=True, cv=5, random_state=101)\n",
    "search_func.fit(X_train, y_train)\n",
    "print ('Best parameters %s' % search_func.best_params_)\n",
    "print ('Cross validation accuracy: mean = %0.3f' % search_func.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forest covertype dataset.\n",
      "\n",
      "A classic dataset for classification benchmarks, featuring categorical and\n",
      "real-valued features.\n",
      "\n",
      "The dataset page is available from UCI Machine Learning Repository\n",
      "\n",
      "    http://archive.ics.uci.edu/ml/datasets/Covertype\n",
      "\n",
      "Courtesy of Jock A. Blackard and Colorado State University.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "covertype_dataset = pickle.load(open( \"covertype_dataset.pickle\", \"rb\" ))\n",
    "print (covertype_dataset.DESCR)\n",
    "covertype_X = covertype_dataset.data[:15000,:]\n",
    "covertype_y = covertype_dataset.target[:15000]\n",
    "covertypes = ['Spruce/Fir', 'Lodgepole Pine', 'Ponderosa Pine', \n",
    "        'Cottonwood/Willow', 'Aspen', 'Douglas-fir', 'Krummholz']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Bagging with weak ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaggingClassifier -> cross validation accuracy: mean = 0.795 std = 0.002\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "hypothesis = BaggingClassifier(KNeighborsClassifier(n_neighbors=1), \n",
    "            max_samples=0.7, max_features=0.7, n_estimators=100)\n",
    "scores = cross_val_score(hypothesis, covertype_X, covertype_y, cv=3, \n",
    "                         scoring='accuracy', n_jobs=-1)\n",
    "print (\"BaggingClassifier -> cross validation accuracy: mean = %0.3f std = %0.3f\" % \n",
    "       (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Random Forests and Extra-Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier -> cross validation accuracy: mean = 0.809 std = 0.009\n",
      "Wall time: 4.75 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "hypothesis = RandomForestClassifier(n_estimators=100, random_state=101)\n",
    "scores = cross_val_score(hypothesis, covertype_X, covertype_y, \n",
    "                         cv=3, scoring='accuracy', n_jobs=-1)\n",
    "print (\"RandomForestClassifier -> cross validation accuracy: mean = %0.3f std = %0.3f\" % \n",
    "       (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesClassifier -> cross validation accuracy: mean = 0.821 std = 0.009\n",
      "Wall time: 4.69 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "hypothesis = ExtraTreesClassifier(n_estimators=100, random_state=101)\n",
    "scores = cross_val_score(hypothesis, covertype_X, covertype_y, cv=3, \n",
    "                         scoring='accuracy', n_jobs=-1)\n",
    "print (\"ExtraTreesClassifier -> cross validation accuracy: mean = %0.3f std = %0.3f\" %\n",
    "       (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier -> cross validation accuracy: mean = -4.642 std = 0.514\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "X_train, y_train = pickle.load(open( \"cadata.pickle\", \"rb\" ))\n",
    "first_rows = 2000\n",
    "\n",
    "X_train = scale(X_train[:first_rows,:].toarray())\n",
    "y_train = y_train[:first_rows]/10**4.\n",
    "hypothesis = RandomForestRegressor(n_estimators=300, random_state=101)\n",
    "scores = cross_val_score(hypothesis, X_train, y_train, cv=3, \n",
    "                         scoring='mean_absolute_error', n_jobs=-1)\n",
    "print (\"RandomForestClassifier -> cross validation accuracy: mean = %0.3f std = %0.3f\" %\n",
    "       (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimating probabilities from an ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n",
    "hypothesis = RandomForestClassifier(n_estimators=100, random_state=101)\n",
    "calibration = CalibratedClassifierCV(hypothesis, method='sigmoid', cv=5)\n",
    "covertype_X = covertype_dataset.data[:15000,:]\n",
    "covertype_y = covertype_dataset.target[:15000]\n",
    "covertype_test_X = covertype_dataset.data[15000:25000,:]\n",
    "covertype_test_y = covertype_dataset.target[15000:25000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis.fit(covertype_X,covertype_y)\n",
    "calibration.fit(covertype_X,covertype_y)\n",
    "prob_raw = hypothesis.predict_proba(covertype_test_X)\n",
    "prob_cal = calibration.predict_proba(covertype_test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_kind = covertypes.index('Ponderosa Pine')\n",
    "probs = pd.DataFrame(list(zip(prob_raw[:,tree_kind],prob_cal[:,tree_kind])), \n",
    "        columns=['raw','calibrted'])\n",
    "plot = probs.plot(kind='scatter', x=0, y=1, s=64, c='blue', edgecolors='white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sequences of models: Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaboost -> cross validation accuracy: mean = 0.622 std = 0.006\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "hypothesis = AdaBoostClassifier(n_estimators=300, random_state=101)\n",
    "scores = cross_val_score(hypothesis, covertype_X, covertype_y, cv=3, \n",
    "                                scoring='accuracy', n_jobs=-1)\n",
    "print (\"Adaboost -> cross validation accuracy: mean = %0.3f std = %0.3f\" %\n",
    "       (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient tree boosting (GTB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "covertype_dataset = pickle.load(open( \"covertype_dataset.pickle\", \"rb\" ))\n",
    "covertype_X = covertype_dataset.data[:15000,:]\n",
    "covertype_y = covertype_dataset.target[:15000] -1 \n",
    "covertype_val_X = covertype_dataset.data[15000:20000,:]\n",
    "covertype_val_y = covertype_dataset.target[15000:20000] -1\n",
    "covertype_test_X = covertype_dataset.data[20000:25000,:]\n",
    "covertype_test_y = covertype_dataset.target[20000:25000] -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=5,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=50,\n",
       "              presort='auto', random_state=101, subsample=1.0, verbose=0,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.cross_validation import cross_val_score, StratifiedKFold\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "hypothesis = GradientBoostingClassifier(max_depth=5,n_estimators=50, random_state=101)\n",
    "hypothesis.fit(covertype_X, covertype_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier -> test accuracy: 0.782\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print ('GradientBoostingClassifier -> test accuracy:', \n",
    "       accuracy_score(covertype_test_y, hypothesis.predict(covertype_test_X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn.cross_validation import cross_val_score, StratifiedKFold\n",
    "import pickle\n",
    "covertype_dataset = pickle.load(open( \"covertype_dataset.pickle\", \"rb\" ))\n",
    "covertype_dataset.target = covertype_dataset.target.astype(int)\n",
    "covertype_X = covertype_dataset.data[:15000,:]\n",
    "covertype_y = covertype_dataset.target[:15000] -1 \n",
    "covertype_val_X = covertype_dataset.data[15000:20000,:]\n",
    "covertype_val_y = covertype_dataset.target[15000:20000] -1\n",
    "covertype_test_X = covertype_dataset.data[20000:25000,:]\n",
    "covertype_test_y = covertype_dataset.target[20000:25000] -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0.1, learning_rate=0.01, max_delta_step=0,\n",
       "       max_depth=24, min_child_weight=1, missing=None, n_estimators=500,\n",
       "       n_jobs=1, nthread=-1, objective='multi:softprob', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=0.9)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "hypothesis = xgb.XGBClassifier(objective= \"multi:softprob\", max_depth = 24, gamma=0.1, subsample = 0.90,\n",
    "                               learning_rate=0.01, n_estimators = 500, nthread=-1)\n",
    "\n",
    "hypothesis.fit(covertype_X, covertype_y, eval_set=[(covertype_val_X, covertype_val_y)], \n",
    "               eval_metric='merror', early_stopping_rounds=25, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anc\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.848\n",
      "[[1512  288    0    0    0    2   18]\n",
      " [ 215 2197   18    0    7   11    0]\n",
      " [   0   17  261    4    0   19    0]\n",
      " [   0    0    4   20    0    3    0]\n",
      " [   1   54    3    0   19    0    0]\n",
      " [   0   16   42    0    0   86    0]\n",
      " [  37    1    0    0    0    0  145]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anc\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "print ('accuracy:', accuracy_score(covertype_test_y, hypothesis.predict(covertype_test_X)))\n",
    "print (confusion_matrix(covertype_test_y, hypothesis.predict(covertype_test_X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with big data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $  Four \\ Point\\ of\\ View\\: $\n",
    "* #### $ volume $\n",
    "* #### $ velocity $\n",
    "* #### $ variety $\n",
    "* #### $ veracity $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating some big datasets as examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Posts inside the data: 11314\n",
      "Average number of words for post: 206\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups_dataset = fetch_20newsgroups(shuffle=True,\n",
    "                                        remove=('headers','footers','quotes'),random_state=6)\n",
    "print ('Posts inside the data: %s' % np.shape(newsgroups_dataset.data))\n",
    "print ('Average number of words for post: %0.0f' % \n",
    "       np.mean([len(text.split(' ')) for text in newsgroups_dataset.data]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "X,y = make_classification(n_samples=10**5, n_features=5, \n",
    "                          n_informative=3, random_state=101)\n",
    "D = np.c_[y,X]\n",
    "np.savetxt('large_dataset_10__5.csv', D, delimiter=\",\") \n",
    "# the saved file should be around 14.6 MB\n",
    "del(D, X, y)\n",
    "\n",
    "X,y = make_classification(n_samples=10**6, n_features=5, \n",
    "                          n_informative=3, random_state=101)\n",
    "D = np.c_[y,X]\n",
    "np.savetxt('large_dataset_10__6.csv', D, delimiter=\",\") \n",
    "# the saved file should be around 146 MB\n",
    "del(D, X, y)\n",
    "\n",
    "X,y = make_classification(n_samples=10**7, n_features=5, \n",
    "                          n_informative=3, random_state=101)\n",
    "D = np.c_[y,X]\n",
    "np.savetxt('large_dataset_10__7.csv', D, delimiter=\",\") \n",
    "# the saved file should be around 1,46 GB\n",
    "del(D, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.remove('large_dataset_10__5.csv')\n",
    "os.remove('large_dataset_10__6.csv')\n",
    "os.remove('large_dataset_10__7.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scalability with volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progressive validation mean accuracy 0.708\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "streaming = pd.read_csv('large_dataset_10__7.csv', header=None, chunksize=10000)\n",
    "learner = SGDClassifier(loss='log')\n",
    "minmax_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "cumulative_accuracy = list()\n",
    "for n,chunk in enumerate(streaming):\n",
    "    if n == 0:\n",
    "            minmax_scaler.fit(chunk.ix[:,1:].values)\n",
    "    X = minmax_scaler.transform(chunk.ix[:,1:].values)\n",
    "    X[X>1] = 1\n",
    "    X[X<0] = 0  \n",
    "    y = chunk.ix[:,0]\n",
    "    if n > 8 :\n",
    "        cumulative_accuracy.append(learner.score(X,y))\n",
    "    learner.partial_fit(X,y,classes=np.unique(y))\n",
    "print ('Progressive validation mean accuracy %0.3f' % np.mean(cumulative_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keeping up with velocity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Classification\n",
    "\n",
    "- sklearn.naive_bayes.MultinomialNB\n",
    "- sklearn.naive_bayes.BernoulliNB\n",
    "- sklearn.linear_model.Perceptron\n",
    "- sklearn.linear_model.SGDClassifier\n",
    "- sklearn.linear_model.PassiveAggressiveClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Regression\n",
    "\n",
    "- sklearn.linear_model.SGDRegressor\n",
    "- sklearn.linear_model.PassiveAggressiveRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier hinge loss : mean accuracy 0.748 in 51.633 secs\n",
      "SGDClassifier log loss : mean accuracy 0.740 in 49.498965 secs\n",
      "Perceptron : mean accuracy 0.674 in 47.902658 secs\n",
      "BernoulliNB : mean accuracy 0.650 in 52.629885 secs\n",
      "PassiveAggressiveClassifier : mean accuracy 0.715 in 49.511889 secs\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "classifiers  = {\n",
    "'SGDClassifier hinge loss' : SGDClassifier(loss='hinge', random_state=101),\n",
    "'SGDClassifier log loss' : SGDClassifier(loss='log', random_state=101),\n",
    "'Perceptron' : Perceptron(random_state=101),\n",
    "'BernoulliNB' : BernoulliNB(),\n",
    "'PassiveAggressiveClassifier' : PassiveAggressiveClassifier(random_state=101)\n",
    "}\n",
    "large_dataset = 'large_dataset_10__6.csv'\n",
    "for algorithm in classifiers:\n",
    "    start = datetime.now()\n",
    "    minmax_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    streaming = pd.read_csv(large_dataset, header=None, chunksize=100)\n",
    "    learner = classifiers[algorithm]\n",
    "    cumulative_accuracy = list()\n",
    "    for n,chunk in enumerate(streaming):\n",
    "        y = chunk.ix[:,0]\n",
    "        X = chunk.ix[:,1:]\n",
    "        if n > 50 :\n",
    "            cumulative_accuracy.append(learner.score(X,y))\n",
    "        learner.partial_fit(X,y,classes=np.unique(y))\n",
    "    elapsed_time = datetime.now() - start\n",
    "    print (algorithm + ' : mean accuracy %0.3f in %s secs' % \n",
    "           (np.mean(cumulative_accuracy),elapsed_time.total_seconds()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dealing with variety"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4141842261\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import murmurhash3_32\n",
    "print (murmurhash3_32(\"something\", seed=0, positive=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last validation score: 0.723\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "def streaming():\n",
    "    for response, item in zip(newsgroups_dataset.target, newsgroups_dataset.data):\n",
    "        yield response, item\n",
    "hashing_trick = HashingVectorizer(stop_words='english', norm = 'l2', non_negative=True)\n",
    "learner = SGDClassifier(random_state=101)\n",
    "texts = list()\n",
    "targets = list()\n",
    "for n,(target, text) in enumerate(streaming()):\n",
    "    texts.append(text)\n",
    "    targets.append(target)\n",
    "    if n % 1000 == 0 and n >0:\n",
    "        learning_chunk = hashing_trick.transform(texts)\n",
    "        if n > 1000:\n",
    "            last_validation_score = learner.score(learning_chunk, targets),\n",
    "        learner.partial_fit(learning_chunk, targets, classes=[k for k in range(20)])\n",
    "        texts, targets = list(), list()\n",
    "print ('Last validation score: %0.3f' % last_validation_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1048576) <class 'scipy.sparse.csr.csr_matrix'>\n",
      "Predicted newsgroup: rec.autos\n"
     ]
    }
   ],
   "source": [
    "New_text = ['A 2014 red Toyota Prius v Five with fewer than 14K miles. Powered by a reliable 1.8L \\\n",
    "            four cylinder hybrid engine that averages 44mpg in the city and 40mpg on the highway.']\n",
    "text_vector = hashing_trick.transform(New_text)\n",
    "print (np.shape(text_vector), type(text_vector))\n",
    "print ('Predicted newsgroup: %s' % newsgroups_dataset.target_names[learner.predict(text_vector)[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 10s 1us/step\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pixels = X_train.shape[1] * X_train.shape[2]\n",
    "n_channels = 1\n",
    "def preprocess(matrix):\n",
    "    return matrix.reshape(matrix.shape[0],n_channels, \n",
    "                          matrix.shape[1],matrix.shape[2]).astype('float32') / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = preprocess(X_train), preprocess(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 1, 28, 28), dtype('float32'), 1.0)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_train.dtype, np.max(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_train.shape[1]\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=(1, 28, 28)))\n",
    "    model.add(Dense(num_pixels, init='normal', activation='relu'))\n",
    "    model.add(Dense(num_classes, init='normal', activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolution_small():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(32, 5, 5, border_mode='valid', input_shape=(1, 28, 28), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolution_large():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(30, 5, 5, border_mode='valid', input_shape=(1, 28, 28), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Convolution2D(15, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With model: baseline\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      " - 13s - loss: 0.2294 - acc: 0.9340 - val_loss: 0.1132 - val_acc: 0.9678\n",
      "Epoch 2/10\n",
      " - 12s - loss: 0.0904 - acc: 0.9734 - val_loss: 0.0829 - val_acc: 0.9736\n",
      "Epoch 3/10\n",
      " - 12s - loss: 0.0567 - acc: 0.9830 - val_loss: 0.0648 - val_acc: 0.9782\n",
      "Epoch 4/10\n",
      " - 12s - loss: 0.0378 - acc: 0.9889 - val_loss: 0.0699 - val_acc: 0.9784\n",
      "Epoch 5/10\n",
      " - 13s - loss: 0.0258 - acc: 0.9927 - val_loss: 0.0775 - val_acc: 0.9770\n",
      "Epoch 6/10\n",
      " - 13s - loss: 0.0202 - acc: 0.9942 - val_loss: 0.0649 - val_acc: 0.9806\n",
      "Epoch 7/10\n",
      " - 13s - loss: 0.0157 - acc: 0.9957 - val_loss: 0.0653 - val_acc: 0.9799\n",
      "Epoch 8/10\n",
      " - 13s - loss: 0.0108 - acc: 0.9971 - val_loss: 0.0638 - val_acc: 0.9821\n",
      "Epoch 9/10\n",
      " - 13s - loss: 0.0097 - acc: 0.9973 - val_loss: 0.0680 - val_acc: 0.9802\n",
      "Epoch 10/10\n",
      " - 13s - loss: 0.0082 - acc: 0.9974 - val_loss: 0.0982 - val_acc: 0.9740\n",
      "Baseline Error: 2.60%\n",
      "\n",
      "With model: small\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      " - 156s - loss: 0.1867 - acc: 0.9452 - val_loss: 0.0616 - val_acc: 0.9812\n",
      "Epoch 2/10\n",
      " - 138s - loss: 0.0628 - acc: 0.9808 - val_loss: 0.0412 - val_acc: 0.9872\n",
      "Epoch 3/10\n",
      " - 143s - loss: 0.0437 - acc: 0.9865 - val_loss: 0.0384 - val_acc: 0.9874\n",
      "Epoch 4/10\n",
      " - 141s - loss: 0.0345 - acc: 0.9888 - val_loss: 0.0376 - val_acc: 0.9866\n",
      "Epoch 5/10\n",
      " - 137s - loss: 0.0262 - acc: 0.9916 - val_loss: 0.0331 - val_acc: 0.9899\n",
      "Epoch 6/10\n",
      " - 138s - loss: 0.0215 - acc: 0.9930 - val_loss: 0.0332 - val_acc: 0.9896\n",
      "Epoch 7/10\n",
      " - 150s - loss: 0.0169 - acc: 0.9943 - val_loss: 0.0402 - val_acc: 0.9875\n",
      "Epoch 8/10\n",
      " - 144s - loss: 0.0138 - acc: 0.9959 - val_loss: 0.0410 - val_acc: 0.9887\n",
      "Epoch 9/10\n",
      " - 142s - loss: 0.0129 - acc: 0.9956 - val_loss: 0.0358 - val_acc: 0.9894\n",
      "Epoch 10/10\n",
      " - 147s - loss: 0.0110 - acc: 0.9963 - val_loss: 0.0383 - val_acc: 0.9889\n",
      "Baseline Error: 1.11%\n",
      "\n",
      "With model: large\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      " - 133s - loss: 0.2745 - acc: 0.9123 - val_loss: 0.0650 - val_acc: 0.9801\n",
      "Epoch 2/10\n",
      " - 131s - loss: 0.0787 - acc: 0.9757 - val_loss: 0.0437 - val_acc: 0.9851\n",
      "Epoch 3/10\n",
      " - 128s - loss: 0.0590 - acc: 0.9815 - val_loss: 0.0409 - val_acc: 0.9867\n",
      "Epoch 4/10\n",
      " - 127s - loss: 0.0478 - acc: 0.9849 - val_loss: 0.0291 - val_acc: 0.9913\n",
      "Epoch 5/10\n",
      " - 130s - loss: 0.0426 - acc: 0.9868 - val_loss: 0.0289 - val_acc: 0.9904\n",
      "Epoch 6/10\n",
      " - 127s - loss: 0.0362 - acc: 0.9886 - val_loss: 0.0295 - val_acc: 0.9904\n",
      "Epoch 7/10\n",
      " - 128s - loss: 0.0335 - acc: 0.9892 - val_loss: 0.0378 - val_acc: 0.9880\n",
      "Epoch 8/10\n",
      " - 127s - loss: 0.0296 - acc: 0.9903 - val_loss: 0.0231 - val_acc: 0.9921\n",
      "Epoch 9/10\n",
      " - 129s - loss: 0.0275 - acc: 0.9910 - val_loss: 0.0261 - val_acc: 0.9916\n",
      "Epoch 10/10\n",
      " - 129s - loss: 0.0246 - acc: 0.9916 - val_loss: 0.0259 - val_acc: 0.9912\n",
      "Baseline Error: 0.88%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')\n",
    "\n",
    "# build the model\n",
    "np.random.seed(101)\n",
    "models = [('baseline', baseline_model()), \n",
    "          ('small', convolution_small()), \n",
    "          ('large', convolution_large())]\n",
    "\n",
    "for name, model in models:\n",
    "    print(\"With model:\", name)\n",
    "    # Fit the model\n",
    "    model.fit(X_train, y_train, validation_data=(X_test, y_test), nb_epoch=10,\n",
    "              batch_size=100, verbose=2)\n",
    "    # Final evaluation of the model\n",
    "    scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(\"Baseline Error: %.2f%%\" % (100-scores[1]*100))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A peek at Natural Language Processing (NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'sexy', 'job', 'in', 'the', 'next', '10', 'years', 'will', 'be', 'statisticians.', 'People', 'think', \"I'm\", 'joking,', 'but', 'who', \"would've\", 'guessed', 'that', 'computer', 'engineers', \"would've\", 'been', 'the', 'sexy', 'job', 'of', 'the', '1990s?']\n"
     ]
    }
   ],
   "source": [
    "my_text = \"The sexy job in the next 10 years will be statisticians. \\\n",
    "People think I'm joking, but who would've guessed that computer engineers \\\n",
    "would've been the sexy job of the 1990s?\"\n",
    "simple_tokens = my_text.split(' ')\n",
    "print (simple_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'sexy', 'job', 'in', 'the', 'next', '10', 'years', 'will', 'be', 'statisticians', '.', 'People', 'think', 'I', \"'m\", 'joking', ',', 'but', 'who', 'would', \"'ve\", 'guessed', 'that', 'computer', 'engineers', 'would', \"'ve\", 'been', 'the', 'sexy', 'job', 'of', 'the', '1990s', '?']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk_tokens = nltk.word_tokenize(my_text)\n",
    "print (nltk_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[':', 'I', 'looove', 'this', 'city', '!', '!', '!', '#love', '#foreverhere']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "\n",
    "tweet = '@mate: I loooooooove this city!!!!!!! #love #foreverhere'\n",
    "tt.tokenize(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'sexy', 'job', 'in', 'the', 'next', '10', 'year', 'wil', 'be', 'stat', '.', 'peopl', 'think', 'i', \"'m\", 'jok', ',', 'but', 'who', 'would', \"'ve\", 'guess', 'that', 'comput', 'engin', 'would', \"'ve\", 'been', 'the', 'sexy', 'job', 'of', 'the', '1990s', '?']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import *\n",
    "stemmer = LancasterStemmer()\n",
    "print ([stemmer.stem(word) for word in nltk_tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('sexy', 'JJ'), ('job', 'NN'), ('in', 'IN'), ('the', 'DT'), ('next', 'JJ'), ('10', 'CD'), ('years', 'NNS'), ('will', 'MD'), ('be', 'VB'), ('statisticians', 'NNS'), ('.', '.'), ('People', 'NNS'), ('think', 'VBP'), ('I', 'PRP'), (\"'m\", 'VBP'), ('joking', 'VBG'), (',', ','), ('but', 'CC'), ('who', 'WP'), ('would', 'MD'), (\"'ve\", 'VBP'), ('guessed', 'VBN'), ('that', 'IN'), ('computer', 'NN'), ('engineers', 'NNS'), ('would', 'MD'), (\"'ve\", 'VBP'), ('been', 'VBN'), ('the', 'DT'), ('sexy', 'JJ'), ('job', 'NN'), ('of', 'IN'), ('the', 'DT'), ('1990s', 'CD'), ('?', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "print (nltk.pos_tag(nltk_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- take: VB (verb, base form)\n",
    "- took: VBD (verb, past tense)\n",
    "- taking: VBG (verb, gerund)\n",
    "- taken: VBN (verb, past participle)\n",
    "- take: VBP (verb, singular present tense)\n",
    "- takes: VBZ (verb, third-person singular present tense)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Elvis/NNP)\n",
      "  (PERSON Aaron/NNP Presley/NNP)\n",
      "  was/VBD\n",
      "  an/DT\n",
      "  (GPE American/JJ)\n",
      "  singer/NN\n",
      "  and/CC\n",
      "  actor/NN\n",
      "  ./.\n",
      "  Born/VBN\n",
      "  in/IN\n",
      "  (GPE Tupelo/NNP)\n",
      "  ,/,\n",
      "  (GPE Mississippi/NNP)\n",
      "  ,/,\n",
      "  when/WRB\n",
      "  (PERSON Presley/NNP)\n",
      "  was/VBD\n",
      "  13/CD\n",
      "  years/NNS\n",
      "  old/JJ\n",
      "  he/PRP\n",
      "  and/CC\n",
      "  his/PRP$\n",
      "  family/NN\n",
      "  relocated/VBD\n",
      "  to/TO\n",
      "  (GPE Memphis/NNP)\n",
      "  ,/,\n",
      "  (GPE Tennessee/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "text = \"Elvis Aaron Presley was an American singer and actor. Born in Tupelo, Mississippi, \\\n",
    "when Presley was 13 years old he and his family relocated to Memphis, Tennessee.\"\n",
    "chunks = nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(text)))\n",
    "print (chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({'on', 'would', 'three', 'whereafter', 'here', 'back', 'ever', 'herself', 'sometime', 'therein', 'was', 'meanwhile', 'whatever', 'along', 'too', 'due', 'as', 'became', 'him', 'through', 'that', 'very', 'do', 'in', 'none', 'most', 'above', 'system', 'when', 'beyond', 'one', 'therefore', 'so', 'we', 'amongst', 'sometimes', 'many', 'seeming', 'towards', 'hereupon', 'be', 'someone', 'until', 'often', 'although', 'find', 'without', 'next', 'them', 'hence', 'within', 'a', 'been', 'onto', 'whether', 'alone', 'thence', 'beside', 'cry', 'fifty', 'five', 'formerly', 'this', 'cannot', 'front', 'inc', 'otherwise', 'six', 'sincere', 'or', 'hasnt', 'two', 'somehow', 'first', 'amount', 'keep', 'noone', 'moreover', 'twenty', 'few', 'thereby', 'nothing', 'except', 'indeed', 'any', 'itself', 'me', 'less', 'well', 'latter', 'beforehand', 'it', 'show', 'some', 'hundred', 'thereupon', 'third', 'throughout', 'seems', 'under', 'both', 'namely', 'please', 'while', 'everything', 'herein', 'an', 'at', 'become', 'latterly', 'somewhere', 'yourselves', 'something', 'mine', 'all', 'for', 'perhaps', 'thru', 'around', 'empty', 'seemed', 'has', 'her', 'call', 'whereupon', 'amoungst', 'hereby', 'themselves', 'after', 'else', 'i', 'nine', 'found', 'only', 'since', 'made', 'nowhere', 'de', 'eight', 'about', 'neither', 'same', 'can', 'least', 'forty', 'of', 'part', 'besides', 'being', 'should', 'against', 'even', 'than', 'which', 'never', 'again', 'take', 'becomes', 'ie', 'more', 'will', 'co', 'bottom', 'fill', 'nor', 'several', 'us', 'the', 'seem', 'un', 'my', 'then', 'behind', 'eleven', 'go', 'thin', 'whoever', 'am', 'he', 'another', 'name', 'becoming', 'rather', 'yet', 'your', 'himself', 'once', 'con', 'already', 'during', 'ten', 'thus', 'upon', 'is', 'must', 'those', 'hers', 'whereas', 'anyway', 'serious', 'wherever', 'off', 'anyone', 'per', 'twelve', 'whole', 'couldnt', 'other', 'yours', 'yourself', 'get', 'they', 'whither', 'everyone', 'nevertheless', 'whom', 'between', 'could', 'hereafter', 'put', 'no', 'who', 'whose', 'are', 'to', 'full', 'whenever', 'with', 'from', 'almost', 'elsewhere', 'whereby', 'fire', 'mill', 'now', 'its', 're', 'among', 'might', 'everywhere', 'four', 'over', 'such', 'also', 'because', 'their', 'much', 'there', 'top', 'how', 'others', 'sixty', 'toward', 'whence', 'where', 'anything', 'describe', 'last', 'however', 'own', 'either', 'ours', 'not', 'down', 'myself', 'nobody', 'thick', 'former', 'eg', 'ourselves', 'together', 'had', 'his', 'thereafter', 'further', 'our', 'move', 'these', 'fifteen', 'what', 'she', 'by', 'out', 'before', 'detail', 'bill', 'if', 'may', 'and', 'mostly', 'enough', 'were', 'done', 'have', 'still', 'give', 'across', 'you', 'etc', 'cant', 'every', 'up', 'below', 'anywhere', 'interest', 'always', 'side', 'but', 'into', 'see', 'though', 'wherein', 'why', 'ltd', 'each', 'afterwards', 'via', 'anyhow'})\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "stop_words = text.ENGLISH_STOP_WORDS\n",
    "print (stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aber', 'alle', 'allem', 'allen', 'aller', 'alles', 'als', 'also', 'am', 'an', 'ander', 'andere', 'anderem', 'anderen', 'anderer', 'anderes', 'anderm', 'andern', 'anderr', 'anders', 'auch', 'auf', 'aus', 'bei', 'bin', 'bis', 'bist', 'da', 'damit', 'dann', 'der', 'den', 'des', 'dem', 'die', 'das', 'daß', 'derselbe', 'derselben', 'denselben', 'desselben', 'demselben', 'dieselbe', 'dieselben', 'dasselbe', 'dazu', 'dein', 'deine', 'deinem', 'deinen', 'deiner', 'deines', 'denn', 'derer', 'dessen', 'dich', 'dir', 'du', 'dies', 'diese', 'diesem', 'diesen', 'dieser', 'dieses', 'doch', 'dort', 'durch', 'ein', 'eine', 'einem', 'einen', 'einer', 'eines', 'einig', 'einige', 'einigem', 'einigen', 'einiger', 'einiges', 'einmal', 'er', 'ihn', 'ihm', 'es', 'etwas', 'euer', 'eure', 'eurem', 'euren', 'eurer', 'eures', 'für', 'gegen', 'gewesen', 'hab', 'habe', 'haben', 'hat', 'hatte', 'hatten', 'hier', 'hin', 'hinter', 'ich', 'mich', 'mir', 'ihr', 'ihre', 'ihrem', 'ihren', 'ihrer', 'ihres', 'euch', 'im', 'in', 'indem', 'ins', 'ist', 'jede', 'jedem', 'jeden', 'jeder', 'jedes', 'jene', 'jenem', 'jenen', 'jener', 'jenes', 'jetzt', 'kann', 'kein', 'keine', 'keinem', 'keinen', 'keiner', 'keines', 'können', 'könnte', 'machen', 'man', 'manche', 'manchem', 'manchen', 'mancher', 'manches', 'mein', 'meine', 'meinem', 'meinen', 'meiner', 'meines', 'mit', 'muss', 'musste', 'nach', 'nicht', 'nichts', 'noch', 'nun', 'nur', 'ob', 'oder', 'ohne', 'sehr', 'sein', 'seine', 'seinem', 'seinen', 'seiner', 'seines', 'selbst', 'sich', 'sie', 'ihnen', 'sind', 'so', 'solche', 'solchem', 'solchen', 'solcher', 'solches', 'soll', 'sollte', 'sondern', 'sonst', 'über', 'um', 'und', 'uns', 'unsere', 'unserem', 'unseren', 'unser', 'unseres', 'unter', 'viel', 'vom', 'von', 'vor', 'während', 'war', 'waren', 'warst', 'was', 'weg', 'weil', 'weiter', 'welche', 'welchem', 'welchen', 'welcher', 'welches', 'wenn', 'werde', 'werden', 'wie', 'wieder', 'will', 'wir', 'wird', 'wirst', 'wo', 'wollen', 'wollte', 'würde', 'würden', 'zu', 'zum', 'zur', 'zwar', 'zwischen']\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words('german'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  A complete data science example: text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "categories = ['sci.med', 'sci.space']\n",
    "to_remove = ('headers', 'footers', 'quotes')\n",
    "twenty_sci_news_train = fetch_20newsgroups(subset='train', remove=to_remove, categories=categories)\n",
    "twenty_sci_news_test = fetch_20newsgroups(subset='test', remove=to_remove, categories=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vect = TfidfVectorizer()\n",
    "X_train = tf_vect.fit_transform(twenty_sci_news_train.data)\n",
    "X_test = tf_vect.transform(twenty_sci_news_test.data)\n",
    "y_train = twenty_sci_news_train.target\n",
    "y_test = twenty_sci_news_test.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy= 0.8848101265822785\n"
     ]
    }
   ],
   "source": [
    "clf = SGDClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print (\"Accuracy=\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_stem_text(text):\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    clean_tokens = [word for word in tokens if word not in stop_words]\n",
    "    stem_tokens = [stemmer.stem(token) for token in clean_tokens]\n",
    "    return \" \".join(stem_tokens)\n",
    "cleaned_docs_train = [clean_and_stem_text(text) for text in twenty_sci_news_train.data]\n",
    "cleaned_docs_test = [clean_and_stem_text(text) for text in twenty_sci_news_test.data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy= 0.889873417721519\n"
     ]
    }
   ],
   "source": [
    "X1_train = tf_vect.fit_transform(cleaned_docs_train)\n",
    "X1_test = tf_vect.transform(cleaned_docs_test)\n",
    "clf.fit(X1_train, y_train)\n",
    "Y1_pred = clf.predict(X1_test)\n",
    "print (\"Accuracy=\", accuracy_score(y_test, Y1_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
